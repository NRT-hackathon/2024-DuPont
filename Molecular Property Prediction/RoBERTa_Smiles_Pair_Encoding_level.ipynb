{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5156376a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoModel\n",
    "from transformers import PreTrainedTokenizer\n",
    "from transformers import AdamW\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import copy\n",
    "from rdkit import Chem\n",
    "from SmilesPE.tokenizer import *\n",
    "import codecs\n",
    "from SmilesPE.learner import *\n",
    "from typing import List, Optional\n",
    "import os\n",
    "import collections\n",
    "from tokenizer_spe import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56406854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94335, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>E_homo</th>\n",
       "      <th>E_lumo</th>\n",
       "      <th>KS_gap</th>\n",
       "      <th>CAN_SMILES</th>\n",
       "      <th>MW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-5.17017</td>\n",
       "      <td>-2.28848</td>\n",
       "      <td>2.88169</td>\n",
       "      <td>C#Cc1[nH]ccc1c1csc2-c3c(C(=O)c12)ccs3</td>\n",
       "      <td>281.352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5.29262</td>\n",
       "      <td>-2.47624</td>\n",
       "      <td>2.81638</td>\n",
       "      <td>O=C1c2c(-c3c1ccs3)scc2c1c2sccc2cc2c1ccs2</td>\n",
       "      <td>380.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-5.30622</td>\n",
       "      <td>-2.40005</td>\n",
       "      <td>2.90617</td>\n",
       "      <td>Cc1sc2-c3sccc3C(=O)c2c1C1=CC(=C)c2c1csc2</td>\n",
       "      <td>338.466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-5.99195</td>\n",
       "      <td>-2.91162</td>\n",
       "      <td>3.08033</td>\n",
       "      <td>O=N(=O)c1cc2c(s1)c(sc2C(F)(F)F)c1csc2-c3c(C(=O...</td>\n",
       "      <td>443.463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.31983</td>\n",
       "      <td>-2.44086</td>\n",
       "      <td>2.87897</td>\n",
       "      <td>Cc1cc2c(-c3c(C2=O)c(cs3)c2cc(ccc2N(=O)=O)c2csc...</td>\n",
       "      <td>409.501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    E_homo   E_lumo   KS_gap  \\\n",
       "0 -5.17017 -2.28848  2.88169   \n",
       "1 -5.29262 -2.47624  2.81638   \n",
       "2 -5.30622 -2.40005  2.90617   \n",
       "3 -5.99195 -2.91162  3.08033   \n",
       "4 -5.31983 -2.44086  2.87897   \n",
       "\n",
       "                                          CAN_SMILES       MW  \n",
       "0              C#Cc1[nH]ccc1c1csc2-c3c(C(=O)c12)ccs3  281.352  \n",
       "1           O=C1c2c(-c3c1ccs3)scc2c1c2sccc2cc2c1ccs2  380.526  \n",
       "2           Cc1sc2-c3sccc3C(=O)c2c1C1=CC(=C)c2c1csc2  338.466  \n",
       "3  O=N(=O)c1cc2c(s1)c(sc2C(F)(F)F)c1csc2-c3c(C(=O...  443.463  \n",
       "4  Cc1cc2c(-c3c(C2=O)c(cs3)c2cc(ccc2N(=O)=O)c2csc...  409.501  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('FULL_VALID_CAN_SMILES_DATASET.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d34b8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data driven Tokenization\n",
    "#SPE Tokenization\n",
    "spe_vob= codecs.open('SPE_data.txt')\n",
    "spe = SPE_Tokenizer(spe_vob)\n",
    "# some default tokens from huggingface\n",
    "default_toks = ['[PAD]', \n",
    "                '[unused1]', '[unused2]', '[unused3]', '[unused4]','[unused5]', '[unused6]', '[unused7]', '[unused8]', '[unused9]', '[unused10]', \n",
    "                '[UNK]', '[CLS]', '[SEP]', '[MASK]']\n",
    "\n",
    "\n",
    "# atom-level tokens \n",
    "atom_tokens = ['O', '=', 'C', '1', '2', '(', 'S', ')', '3', '4', '[NH]', '#', '5', '6', 'F',\n",
    "                '[N]', '\\\\', '/', '[S]', 'N', '[Se]', '[CH2]', '[C]', '[CH]', '[SiH2]', '7', '8', 'Br', 'Cl', \n",
    "                '[P]', '[N+]', '[O-]', '[SH]', '9', '%10', '%11', '%12', '[N-]', 'P', ':', '[PH]', '[C-]', '[NH-]',\n",
    "                'c', 's', 'n', 'o','[se]', '[nH]', '[GeH2]', '[C@@H]', '[C@@]', '[C@H]', '[S@]', '[S@@]', '[c]', '[C@]', '[n]']\n",
    "# spe tokens\n",
    "with open('SPE_data.txt', \"r\") as ins:\n",
    "    spe_toks = []\n",
    "    for line in ins:\n",
    "        spe_toks.append(line.split('\\n')[0])\n",
    "\n",
    "spe_tokens = []\n",
    "for s in spe_toks:\n",
    "    spe_tokens.append(''.join(s.split(' ')))\n",
    "    \n",
    "spe_vocab = default_toks + atom_tokens + spe_tokens\n",
    "len(spe_vocab)\n",
    "with open('vocab_spe.txt', 'w') as f:\n",
    "    for voc in spe_vocab:\n",
    "        f.write(f'{voc}\\n')\n",
    "spe_tokenizer = SMILES_SPE_Tokenizer(vocab_file='vocab_spe.txt', spe_file= 'SPE_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2d4ffcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [12, 75, 45, 123, 278, 155, 42, 13], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smi_1 = 'CC[N+](C)(C)Cc1ccccc1Br'\n",
    "smi_2 = 'c1cccc1[invalid]'\n",
    "encoded_input = spe_tokenizer(smi_1)\n",
    "encoded_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7f0e025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spe_encode_smiles(smiles):\n",
    "    try:\n",
    "        encoded = spe_tokenizer(smiles)\n",
    "        return encoded['input_ids']\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding SMILES {smiles}: {str(e)}\")\n",
    "        return None\n",
    "df['data_driven_encoded'] = df['CAN_SMILES'].apply(spe_encode_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4818f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmilesDataset(Dataset):\n",
    "    def __init__(self, encodings, properties):\n",
    "        self.encodings = encodings  \n",
    "        self.properties = properties  \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.properties)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {\n",
    "            'input_ids': torch.tensor(self.encodings[idx], dtype=torch.long),\n",
    "            'attention_mask': torch.tensor([1]*len(self.encodings[idx]), dtype=torch.long)\n",
    "        }\n",
    "        item['properties'] = torch.tensor(self.properties[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "\n",
    "# Get the input and output data\n",
    "spe_X = list(df['data_driven_encoded'])  \n",
    "y = df[['E_homo', 'E_lumo']].values  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(spe_X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#Normalize properties\n",
    "scaler = StandardScaler()\n",
    "y_train_scaled = scaler.fit_transform(y_train)  \n",
    "y_test_scaled = scaler.transform(y_test) \n",
    "\n",
    "\n",
    "# Convert the lists of integers to tensors\n",
    "train_encodings = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in X_train], batch_first=True)\n",
    "test_encodings = torch.nn.utils.rnn.pad_sequence([torch.tensor(seq) for seq in X_test], batch_first=True)\n",
    "\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = SmilesDataset(train_encodings, y_train_scaled)\n",
    "test_dataset = SmilesDataset(test_encodings, y_test_scaled)\n",
    "\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "835bba31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akepa\\anaconda3\\Lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "class ChemBERTaForPropertyPrediction(torch.nn.Module):\n",
    "    def __init__(self, model_checkpoint, num_properties):\n",
    "        super().__init__()\n",
    "        self.chemberta = AutoModel.from_pretrained(model_checkpoint)\n",
    "        self.regressor = torch.nn.Linear(self.chemberta.config.hidden_size, num_properties)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.chemberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "        return self.regressor(pooled_output)\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "spe_model = ChemBERTaForPropertyPrediction('seyonec/ChemBERTa-zinc-base-v1', num_properties=2).to(device)\n",
    "spe_optimizer = AdamW(spe_model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5928caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, properties = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['properties'].to(device)\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = torch.nn.functional.mse_loss(outputs, properties)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids, attention_mask, properties = batch['input_ids'].to(device), batch['attention_mask'].to(device), batch['properties'].to(device)\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = torch.nn.functional.mse_loss(outputs, properties)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c369b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akepa\\AppData\\Local\\Temp\\ipykernel_18600\\2154977056.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'input_ids': torch.tensor(self.encodings[idx], dtype=torch.long),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train loss: 0.1934343444032657, Validation loss: 0.1930637525299848\n",
      "Epoch 1, Train loss: 0.15708855459022036, Validation loss: 0.1484851081745099\n",
      "Epoch 2, Train loss: 0.13373051101958514, Validation loss: 0.15044662804674294\n",
      "Epoch 3, Train loss: 0.11906345276104388, Validation loss: 0.13252606624263827\n",
      "Epoch 4, Train loss: 0.10461725329963673, Validation loss: 0.1272754873007031\n",
      "Epoch 5, Train loss: 0.09587433754816657, Validation loss: 0.12596180329762272\n",
      "Epoch 6, Train loss: 0.08839500935265779, Validation loss: 0.13341902753938054\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(spe_model, train_loader,spe_optimizer,device)\n",
    "    val_loss = evaluate(spe_model, test_loader,device)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch}, Train loss: {train_loss}, Validation loss: {val_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe72028",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'spe_model_state_dict': spe_model.state_dict(),\n",
    "            'spe_optimizer_state_dict': spe_optimizer.state_dict(),\n",
    "            }, 'ChemBERTa_spe_Level.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becb2710",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_save(model, data_loader, scaler, device, filename):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            properties = batch['properties'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            predictions.append(outputs.cpu().numpy())\n",
    "            actuals.append(properties.cpu().numpy())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    actuals = np.concatenate(actuals, axis=0)\n",
    "    \n",
    "    # Inverse transform of the scaled properties\n",
    "    y_pred = scaler.inverse_transform(predictions)\n",
    "    y_actual = scaler.inverse_transform(actuals)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_actual, y_pred)\n",
    "    r2 = r2_score(y_actual, y_pred)\n",
    "    \n",
    "    # Save test predictions and actual values to CSV\n",
    "    test_results = pd.DataFrame({\n",
    "        'E_homo_actual': y_actual[:, 0],\n",
    "        'E_lumo_actual': y_actual[:, 1],\n",
    "        'E_homo_predicted': y_pred[:, 0],\n",
    "        'E_lumo_predicted': y_pred[:, 1]\n",
    "    })\n",
    "    test_results.to_csv(filename, index=False)\n",
    "    \n",
    "    return (y_actual, y_pred, mse, r2)\n",
    "\n",
    "spe_model.to(device)  \n",
    "evaluate_and_save(char_model, char_test_loader, scaler, device, 'spe_level_chemberta_results.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
